{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2주차]_기본과제_주어진_문장에서_나올_다음_단어를_예측하는_모델_구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJhn6J6yqT4v"
   },
   "source": [
    "질문\n",
    "1. collate_fn 함수에서 labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3]) 이걸 왜 하는 건가요? label을 input_ids에서 뒤에서 3번째 토큰을 사용하는 이유를 모르겠습니다.\n",
    "\n",
    "2. decoder 부분이 없는거 같은데, transformer라고 볼 수 있나요?\n",
    "\n",
    "3. task가 분류가 아닌데 CrossEntropyLoss를 사용해도 되나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHsHvUvJxwDd",
    "outputId": "9f74a35c-7c17-46b7-923c-bed83de4310f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, sacremoses, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 sacremoses-0.1.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554,
     "referenced_widgets": [
      "764c8d54ec114de7bf0b4767534772b3",
      "50bbcbf227714eadbf17e5aab333fb35",
      "9f3bfbd595464906837f0b809029fee4",
      "48b644dd960647a8990b1565ed591bdf",
      "2261acb20df84316b6dd143470334bce",
      "60b56a990f5948479867d321d3cbf6a3",
      "afe0ab4ac30e4c26afb9b4b5e96b8da6",
      "ec3fe3c0199f4bce8d4b721394d16e21",
      "c90cca1bbf9d4d34b82ecb034674be24",
      "1bbd15f5ad7a42f487f6328243b85a82",
      "ed5e98f1f6f94324ab9548b2fed7ecb3",
      "61025fbc5ace49bb863a65530b0b7bb8",
      "d3109e149be4452b84b0d0f716ef9800",
      "e652ad032e9847be8666fe5a45267e43",
      "bbfc3390803b435681ebe891ab785a5c",
      "1feed0e0f2374f9c88a656dec719aff9",
      "fbd21fb1af7047bdbad2f3664b042c08",
      "c2889f0fc72b48c0a8fe118eb0609d4d",
      "f047fdd0a6b94917a1af30bf979b72e8",
      "5ba1b7cffa3d4197a61c889a6e3959a5",
      "885c470d8e044a6cb66e05214e2efd22",
      "7c939f4051974df2846fffb7501dae8b",
      "9811ff10bebd43cc823004136b3b2c6c",
      "137a1f27901a4be18b787721435c03a0",
      "fbafbd74aabe4bbbbc03fd500d431ac3",
      "14671e7614374fceaf3f389f419fb8bb",
      "f8b008b670f8403892a905c211b3fd02",
      "2341104c8f8f482e98b66b91a7ba902f",
      "f91410de5ebf427abed8403c769033f2",
      "00e574c65f6941ccb9a010473c6254f6",
      "abd8aac19da14959876cfdfeb2805cb9",
      "9ddb81250a8a498d88ca46dfb0e2d388",
      "9c97431bd608425485a7153d6340adf4",
      "b719e16a22ca4c4591af9c0b2aecb36a",
      "06fdc0bc3a2045bfba1440a4b25778f4",
      "099f24027540421185da1d4fee9de577",
      "b59b5591cd9b4efd94749020ecd68c17",
      "aa51e316850f460cb00e1641e408dd79",
      "ee5903e3e0534c87b9cf3ad11e65f55e",
      "5eb3d4ea64f244579f67057403f71dd5",
      "4638b97aa327435aad5583b0e7864ab9",
      "4e5584ed38664d8988677345e2b78527",
      "4f95c1ac344b49448f92a4cd11dd4859",
      "0a1d74d974294a71a49ad8559dc5bcfa",
      "6142d963109741abbd886e33961d1ae4",
      "1239d01d90d94ca996e07099b90f6ef6",
      "cd5998e291af4fd890ac043a8da9b04e",
      "165458a931c7414196b3d3242ede8a91",
      "4b601f0a5058416a9f4939b00b7c12c4",
      "a9d2954f06734ebc85e58b85fb3d28f0",
      "7fc2610902b04c3aa309de5cffa8f1c3",
      "901506efd22b4c13aa521c491def20be",
      "42fd9352b0a541d6adbff53053f04de5",
      "cc725a4f199841ffa3e52fe64f978443",
      "8e3820e29bbb4d87bc6fc52107cc9ecd",
      "97aba8900d3a4f8aa01f5bf0ec402b4c",
      "bf99ea6092cf4d2abfb4a6e38985673c",
      "a58f590c2ebb4e1eb9a29bfed38c3413",
      "84550616f9774dc1af525d30191fc954",
      "691a28e6fa6940a6b1dc7302ab730cda",
      "8b1be9727a82439faff3d07b5d15b70e",
      "3cc4e9e5330542d590190127289ab04d",
      "ded78148aa4f4fc293b13f94c369cb53",
      "2c3358d905004e759431f2489748cd01",
      "357b9326929247ba861a3d495e42723f",
      "8d03bcc154ea4d1aa0bc3236362d17ff",
      "bf8717f4646947eca5cc7c11c7f484a1",
      "570f94f2a29a42e3856945c96be3bdba",
      "b40977b6f70043b8b7d741113a7fd3e6",
      "60e9998709a945aa85992cc4754001d6",
      "1b478bd3874a472bbf234df2473dd8eb",
      "5b977a7cd74b4fc3bf9e2fb7d196d84b",
      "e77f753431f64fc6b217be580490465a",
      "43f1341e5f4844ffa6a739e9c796ff28",
      "51fc4d28085d487c8d9f80d4ef1f5ffa",
      "be084781acdb4cf3a57bdc0f20f44c58",
      "efbfc430922c47d6a1016a47af436bb4",
      "7f5c25e7ba4d40a0beac63a6ff5f72d5",
      "5c00546e4df2476daa7c27dc928ff6cb",
      "5bc5ffd2b142469396df7d6fc0f7008d",
      "663652cadd1b41ea921989d1708ca713",
      "1b63b99ebef9459bb357e2df2d451101",
      "f62cd4b1477244f8a33bca5d43ebafef",
      "c1378b0f2a674b2e90d1a3d48283ed9a",
      "8735e788ba5a4fea88a6f8ba51956898",
      "dcc8d652d4eb463ea5e0f7c12362b247",
      "99b2e5c18a174cfc9861dee420abc222",
      "35f823a9f83d4a68ae4a52a4279fc91d",
      "a055e7dd20e6442090ecf718825ebf02",
      "88444881b7a0446981fc451ac7a42cdc",
      "5d2ca1f148634e80aa753582768e270f",
      "5d23846282d64f28801efca5d8a31dad",
      "95936cfa078c4ef9b7602f89f35f67d9",
      "a25f614567ff4fc8a799d02bbf436a7f",
      "f030aaa82d6a403796c60b585398a24c",
      "d3a28c166f0c44edada855e4e4547df1",
      "24920dcff9c64fc3998bb01f24e34c99",
      "ae51d7970180427e8777c04e8309584f",
      "4cc0e28ea44446e6ab4e6376dbb39763",
      "d9dd3dbaf7e549398d414d49175c2fed",
      "26bf6a2918614211b1a7f360984dbdc4",
      "ece2a26327a74074b1e731294dd804d5",
      "2c41f693858248fb9804eb6cfd9d1563",
      "4c898a514dc9429e8af81fc8757554f6",
      "4dc0d7ca5bed4b34a79f9304da6f70cd",
      "ee4e1804f0dc4e438647408fa0c2d68f",
      "726a5020842945b1bacdaba40dfbbca4",
      "5bd8845473e94157ac71c5076ed51435",
      "4cebde5ad4724147a412efae997b5cb1",
      "201ad069987e4033ac7505d30680abce",
      "02551cd9f2b24c65aa0a9948d6ba998e",
      "332de3fbc590474f9e6f6375c19eab77",
      "fc78ae6130c243f48832eead995e96ec",
      "f1846befd482436c830dd66e92282add",
      "011c7a22a003409b874563a7d00e9651",
      "f2aa5108369043cc851f0ffca6bae4d6",
      "f0196832cc034c34ab06a2a72735982a",
      "4cac7c50e07c4f0a9182f1adb0535c09",
      "3d9188898f784ff3a45b70207b830b8e",
      "203f6a69bff34165973707db068f266a",
      "76b3c7aa859a45ee8ea33800146c86e3"
     ]
    },
    "id": "KQlOAh6dxwDf",
    "outputId": "1a9cc02e-9d51-446b-e62a-64fdff2ff578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764c8d54ec114de7bf0b4767534772b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61025fbc5ace49bb863a65530b0b7bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9811ff10bebd43cc823004136b3b2c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b719e16a22ca4c4591af9c0b2aecb36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6142d963109741abbd886e33961d1ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aba8900d3a4f8aa01f5bf0ec402b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8717f4646947eca5cc7c11c7f484a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/huggingface/pytorch-transformers/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5c25e7ba4d40a0beac63a6ff5f72d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a055e7dd20e6442090ecf718825ebf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dd3dbaf7e549398d414d49175c2fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02551cd9f2b24c65aa0a9948d6ba998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n",
    "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")\n",
    "\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3]) # input_ids에서 뒤에서 3번째 토큰을 사용. why..?\n",
    "    texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-3]))\n",
    "\n",
    "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)  # 입력 시퀀스들을 패딩하여 동일한 길이로 맞춤\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DB0OvCB6KA7"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim  # 입력 벡터의 차원\n",
    "    self.d_model = d_model      # 전체 모델 차원 (multi-head attention 출력 차원)\n",
    "    self.n_heads = n_heads      # 어텐션 헤드의 수\n",
    "\n",
    "    # 쿼리, 키, 밸류 생성을 위한 선형 변환 레이어들\n",
    "    self.wq = nn.Linear(input_dim, d_model)\n",
    "    self.wk = nn.Linear(input_dim, d_model)\n",
    "    self.wv = nn.Linear(input_dim, d_model)\n",
    "    # 멀티헤드 출력을 다시 하나로 합친 후 사용하는 출력 선형 레이어\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # 어텐션 스코어에 소프트맥스를 적용하기 위한 함수\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # 입력 x를 쿼리, 키, 밸류로 선형 변환 (B: 배치, S: 시퀀스 길이, D: 차원)\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "    B, S, D = q.shape[0], q.shape[1], self.d_model // self.n_heads # 각 헤드당 차원 계산\n",
    "\n",
    "    # (B, S, d_model) → (B, n_heads, S, D): 멀티헤드 형태로 변환\n",
    "    q = q.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
    "    k = k.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
    "    v = v.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
    "\n",
    "    # 어텐션 스코어 계산: Q * K^T → (B, n_heads, S, S)\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, H, S, D) * (B, H, D, S) = (B, H, S, S)\n",
    "    score = score / sqrt(self.d_model) # 어텐션 스코어 정규화 (스케일드 어텐션)\n",
    "\n",
    "    # 마스크가 있다면, 마스킹된 위치에 매우 작은 값(-1e9)을 더해 softmax 후 영향 없도록 함\n",
    "    if mask is not None:\n",
    "      score = score + (mask[:, None] * -1e9)\n",
    "\n",
    "    # 소프트맥스 적용 → 어텐션 확률로 변환\n",
    "    score = self.softmax(score)\n",
    "    # 어텐션 확률을 값 벡터 V에 곱함 → (B, n_heads, S, D)\n",
    "    result = torch.matmul(score, v)  # (B, H, S, D)\n",
    "\n",
    "    # (B, n_heads, S, D) → (B, S, d_model): 헤드 차원 합치기\n",
    "    result = result.transpose(1, 2).reshape((B, S, -1))\n",
    "    # 최종 출력 선형 레이어 통과\n",
    "    result = self.dense(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZHPCn9AS5Gp"
   },
   "outputs": [],
   "source": [
    "# Transformer 인코더 레이어 구현\n",
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, n_heads, dff):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim  # 입력 차원\n",
    "    self.d_model = d_model      # 모델 차원\n",
    "    self.n_heads = n_heads      # 어텐션 헤드 수\n",
    "    self.dff = dff              # FFN의 내부 차원\n",
    "\n",
    "    # 다중 헤드 어텐션 모듈\n",
    "    self.sa = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "    # Position-wise Feed Forward Network (FFN)\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(d_model, dff),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dff, d_model)\n",
    "    )\n",
    "\n",
    "    # 첫 번째 LayerNorm (어텐션 후)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "    # 두 번째 LayerNorm (FFN 후)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # 어텐션 출력\n",
    "    x1 = self.sa(x, mask)\n",
    "    x1 = self.dropout1(x1)\n",
    "    # 어텐션 결과 + 입력 → Residual + Norm\n",
    "    x1 = self.norm1(x + x1)\n",
    "\n",
    "    # FFN 적용\n",
    "    x2 = self.ffn(x1)\n",
    "    x2 = self.dropout2(x2)\n",
    "    # FFN 결과 + Residual + Norm\n",
    "    x2 = self.norm2(x1 + x2)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uf_jMQWDUR79",
    "outputId": "3b170cba-55c0-4ed6-d48f-181c839bd980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 위치(pos)와 차원(i), 모델 차원(d_model)을 받아 각 위치에 대한 주기적인 각도 값(angle)을 계산\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # 짝수 인덱스(0, 2, 4...)는 sin 적용\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) # 홀수 인덱스(1, 3, 5...)는 cos 적용\n",
    "    pos_encoding = angle_rads[None, ...] # (1, position, d_model) 형태로 차원을 하나 추가하여 배치 차원 포함\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "\n",
    "max_len = 400\n",
    "print(positional_encoding(max_len, 256).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, n_heads, n_layers, dff):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vocab_size = vocab_size         # 전체 단어 집합 크기\n",
    "    self.d_model = d_model               # 임베딩 및 모델 차원\n",
    "    self.n_heads = n_heads               # 멀티헤드 어텐션의 헤드 수\n",
    "    self.n_layers = n_layers             # Transformer 인코더 층 수\n",
    "    self.dff = dff                       # FFN 내부 차원\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model) # 임베딩 레이어 (토큰 → 벡터)\n",
    "    # 포지셔널 인코딩 (학습되지 않음, sin/cos 기반)\n",
    "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
    "    # Transformer 인코더 레이어들을 리스트로 저장\n",
    "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, n_heads, dff) for _ in range(n_layers)])\n",
    "    self.classification = nn.Linear(d_model, len(tokenizer)) # 분류를 위한 출력 레이어\n",
    "\n",
    "  def forward(self, x):\n",
    "    seq_len = x.shape[1] # 입력 시퀀스 길이 추출\n",
    "    mask = (x == tokenizer.pad_token_id)[..., None] # (B, S, 1)\n",
    "\n",
    "    # 임베딩 적용 후 스케일 조정\n",
    "    x = self.embedding(x)\n",
    "    x = x * sqrt(self.d_model)\n",
    "    x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "    # 모든 Transformer 인코더 레이어를 통과\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "\n",
    "    x = x[:, 0]\n",
    "    x = self.classification(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, 4, 5, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHVVsWBPQmnv"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to('cuda')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSSDAv8lRV5z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "    preds = model(inputs)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "al_b56TYRILq",
    "outputId": "7bb4ca40-6fa8-4ef7-afa9-87a2d54f8c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 348.96052265167236\n",
      "=========> Train acc: 0.000 | Test acc: 0.000\n",
      "Epoch   1 | Train Loss: 227.64956188201904\n",
      "=========> Train acc: 0.000 | Test acc: 0.000\n",
      "Epoch   2 | Train Loss: 105.80481481552124\n",
      "=========> Train acc: 0.000 | Test acc: 0.000\n",
      "Epoch   3 | Train Loss: 13.725701160728931\n",
      "=========> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch   4 | Train Loss: 0.7626572623848915\n",
      "=========> Train acc: 1.000 | Test acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "    preds = model(inputs)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIVa0mWfqfPU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
